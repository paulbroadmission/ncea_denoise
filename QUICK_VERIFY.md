# Quick Verify: One-Command Verification

## TL;DR: Single Command

```bash
cd /Users/paul/prj/GenAI/vibe/paper_machine/auto-research-fleet
source venv/bin/activate
python3 workspace/src/verify_everything.py --dataset BETA
```

**Expected Output**: `‚úì ALL VERIFICATION PHASES PASSED!`

---

## What It Does in 3 Minutes

```
Your Script
    ‚Üì
Phase 1: Guardian Checks (7)
‚îú‚îÄ Imports work?
‚îú‚îÄ Config valid?
‚îú‚îÄ Model builds?
‚îú‚îÄ Loss functions compute?
‚îú‚îÄ Data loads?
‚îú‚îÄ Training step works?
‚îî‚îÄ CONFIG-SYNC present?
    ‚Üì
Phase 2: Data Integrity (8)
‚îú‚îÄ Data shape correct?
‚îú‚îÄ All-zeros detection
‚îú‚îÄ Constant-values detection
‚îú‚îÄ Signal statistics realistic?
‚îú‚îÄ Noise present?
‚îú‚îÄ Trials different?
‚îú‚îÄ Class distribution valid?
‚îî‚îÄ No data leakage?
    ‚Üì
Phase 3: Forbidden Checks (6)
‚îú‚îÄ Synthetic as real? (NO)
‚îú‚îÄ 100% accuracy on real? (NO)
‚îú‚îÄ Data leakage? (NO)
‚îú‚îÄ Cherry-picking? (NO)
‚îú‚îÄ Metric mismatch? (NO)
‚îî‚îÄ Missing evidence? (NO)
    ‚Üì
DECISION
‚úì GO FOR TRAINING  or  ‚úó FIX ISSUES
    ‚Üì
JSON Audit Trail Saved
```

---

## Usage Examples

### Local Test with Synthetic Data
```bash
python3 workspace/src/verify_everything.py --dataset synthetic
```

### Before Real Training
```bash
python3 workspace/src/verify_everything.py --dataset BETA
```

### On Colab
```python
import subprocess

result = subprocess.run(
    ["python3", "workspace/src/verify_everything.py", "--dataset", "BETA"],
    cwd="/content/auto-research-fleet",
    capture_output=True,
    text=True
)

print(result.stdout)
assert "ALL VERIFICATION PHASES PASSED" in result.stdout, "Verification failed!"
```

---

## Expected Output

### Success ‚úÖ
```
======================================================================
  COMPREHENSIVE INTEGRITY VERIFICATION SYSTEM
======================================================================

Starting full verification for BETA dataset...

======================================================================
  PHASE 1: Guardian Pre-Flight Checks
======================================================================

[1/7] Imports...
  ‚úì PASS All modules importable
[2/7] Configuration...
  ‚úì PASS Config valid (epochs=100, batch=32, lr=0.001)
[3/7] Model Architecture...
  ‚úì PASS Model created (546,128 params)
[4/7] Loss Functions...
  ‚úì PASS Loss functions compute
[5/7] Data Loading...
  ‚úì PASS Data loading works
[6/7] Training Step...
  ‚úì PASS Training step successful
[7/7] CONFIG-SYNC Consistency...
  ‚úì PASS CONFIG-SYNC tags found

======================================================================
  PHASE 2: Data Integrity Verification
======================================================================

[Validating BETA data...]

  ‚úì PASS Data integrity checks:
    - Shape valid: True
    - All-zeros check: PASS
    - Signal std: 0.987 (valid: >0.01)
    - No data leakage

======================================================================
  PHASE 3: Forbidden Checks (Fraud Prevention)
======================================================================

[1/3] Data Source Verification
  ‚úì PASS Data source valid (BETA)
[2/3] Hallucination Detection
  ‚ìò INFO: Will check after training results available
[3/3] Data Leakage Prevention
  ‚úì PASS No data leakage detected

======================================================================
‚úì ALL VERIFICATION PHASES PASSED!
======================================================================

‚úì Audit trail saved to verification_audit_trail.json
```

### Failure ‚ùå
```
======================================================================
‚úó VERIFICATION FAILED AT PHASE 2 (Data Integrity)
======================================================================

Data integrity check failed:
‚úó FAIL Signal std too low (0.000009), likely FAKE

Fix the issue and try again!
```

---

## Audit Trail Output

Every run generates `verification_audit_trail.json`:

```json
{
  "timestamp": "2026-02-15T15:26:43",
  "dataset": "BETA",
  "device": "cuda",
  "phases": {
    "guardian": {"status": "PASS", "checks": 7, "passed": 7},
    "data_integrity": {"status": "PASS", "signal_std": 0.987},
    "forbidden": {"status": "PASS", "checks": ["synthetic_as_real", "data_leakage"]}
  },
  "summary": {
    "passed_checks": 9,
    "failed_checks": 0,
    "status": "PASS"
  }
}
```

---

## What Each Phase Checks

### Phase 1: Guardian (7 checks)
Validates that your **CODE IS CORRECT**

| Check | What | Fail If |
|-------|------|---------|
| Imports | All modules loadable | Missing dependency |
| Config | Hyperparameters valid | Invalid value |
| Model | Builds & runs | Shape mismatch |
| Loss | Computes & differentiates | NaN/Inf values |
| Data | Loads correctly | Shape wrong |
| Training | Forward‚ÜíBackward‚ÜíUpdate works | Gradient error |
| CONFIG-SYNC | Documentation present | Missing tags |

### Phase 2: Data Integrity (8 checks)
Validates that your **DATA IS REAL**

| Check | Detects | Fails If |
|-------|---------|----------|
| Shape | Correct dimensions | Wrong shape |
| All-zeros | Fake data (all 0s) | All zeros |
| Constants | Synthetic data | Constant values |
| Std | Signal variation | std < 0.01 |
| Range | Signal spread | range < 0.1 |
| Correlation | Duplicate trials | corr > 0.99 |
| Noise | Real noise present | noise < 0.001 |
| Classes | Balanced distribution | <5 samples/class |

### Phase 3: Forbidden Checks (6 checks)
Enforces **ZERO-TOLERANCE POLICY**

| Check | Forbids | Fails If |
|-------|---------|----------|
| Synthetic | Claiming synthetic as real | data_source mismatch |
| Leakage | Train/test overlap | Any shared samples |
| Hallucinate | 100% on real data | accuracy == 1.0 |
| Consistency | Metric mismatch | \|Acc - F1\| > 0.10 |
| Cherry-pick | Best-run only | max - mean > 0.05 |
| Evidence | Missing logs/checkpoint | No saved model |

---

## Quick Troubleshooting

| Error | Cause | Fix |
|-------|-------|-----|
| Module not found | Dependency missing | `pip install torch numpy scipy scikit-learn` |
| CUDA not available | GPU not installed | Falls back to CPU automatically |
| Data shape wrong | Corrupted data | Download fresh from BETA/OpenBMI |
| All zeros detected | Fake data | Use real data source |
| Training step fails | Config issue | Check config.py parameters |
| Data leakage detected | Train/test overlap | Fix split logic in data.py |

---

## The Workflow

### Before Local Testing
```bash
python3 verify_everything.py --dataset synthetic
# ‚úì PASS ‚Üí OK to test locally
# ‚úó FAIL ‚Üí Fix code first
```

### Before Real Training
```bash
python3 verify_everything.py --dataset BETA
# ‚úì PASS ‚Üí OK to train on GPU
# ‚úó FAIL ‚Üí Fix issues first
```

### Before Publishing
```bash
python3 verify_everything.py --dataset BETA
cp verification_audit_trail.json results/
# Include with paper: proof of validation
```

---

## Key Points

1. **Run BEFORE every training** ‚Äî prevents wasted GPU time
2. **Takes 3 minutes** ‚Äî worth it vs. hours of broken training
3. **Clear output** ‚Äî easy to understand pass/fail
4. **JSON audit trail** ‚Äî documents validation conditions
5. **Zero tolerance** ‚Äî fraud detection built-in
6. **Device fallback** ‚Äî works on CPU or GPU
7. **No configuration** ‚Äî works out-of-the-box

---

## Next Steps After Verification Passes

### If Synthetic: Test Locally
```bash
python3 main.py --mode train --dataset synthetic --epochs 2
```

### If BETA: Train on Colab
1. Open `COLAB_GPU_SETUP.md`
2. Copy notebook to Colab
3. Run verification first
4. Then train

### Save Results with Audit Trail
```bash
cp verification_audit_trail.json results/
# Keep with results for reproducibility
```

---

**Remember**: This is your first line of defense. Always run verification before training!

üõ°Ô∏è One command. Three layers of protection. Zero tolerance for fraud.
