{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Neural Conditional Ensemble Averaging ‚Äî Colab GPU Training\n",
    "\n",
    "**Complete workflow with Guardian validation and GPU acceleration**\n",
    "\n",
    "This notebook:\n",
    "1. ‚úÖ Clones code from GitHub\n",
    "2. ‚úÖ Runs Guardian pre-flight validation\n",
    "3. ‚úÖ Trains on GPU (T4/V100/A100)\n",
    "4. ‚úÖ Saves results to Google Drive\n",
    "5. ‚úÖ Shows final metrics and plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup ‚Äî Mount Drive & Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository from GitHub\n",
    "!git clone https://github.com/paulbroadmission/ncea_denoise.git /content/ncea_denoise\n",
    "%cd /content/ncea_denoise\n",
    "!git log --oneline -1\n",
    "print(\"‚úÖ Repository cloned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "    print(f\"‚úÖ GPU Memory: {gpu_mem_gb:.2f} GB\")\n",
    "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ùå NO GPU DETECTED!\")\n",
    "    print(\"Go to: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    raise RuntimeError(\"GPU required for training\")\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "print(f\"\\n‚úÖ Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements (quiet mode)\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q numpy scipy scikit-learn matplotlib seaborn pandas tqdm tensorboard\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: üõ°Ô∏è RUN GUARDIAN VALIDATION (Pre-flight Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GUARDIAN VALIDATION ‚Äî Pre-flight checks\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python3\", \"workspace/src/guardian.py\"],\n",
    "    cwd=\"/content/ncea_denoise\",\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"‚ùå GUARDIAN FAILED\")\n",
    "    print(result.stderr)\n",
    "    raise RuntimeError(\"Guardian validation failed. Fix issues and retry.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ GUARDIAN PASSED ‚Äî All checks passed!\")\n",
    "    print(\"Ready for training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Setup Colab Paths & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/ncea_denoise/workspace/src')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('/content/drive/MyDrive/ncea_results', exist_ok=True)\n",
    "os.makedirs('/content/ncea_denoise/workspace/checkpoints', exist_ok=True)\n",
    "os.makedirs('/content/ncea_denoise/workspace/logs', exist_ok=True)\n",
    "\n",
    "RESULTS_DIR = '/content/drive/MyDrive/ncea_results'\n",
    "CHECKPOINT_DIR = '/content/ncea_denoise/workspace/checkpoints'\n",
    "\n",
    "print(\"‚úÖ Colab paths configured\")\n",
    "print(f\"   Results dir: {RESULTS_DIR}\")\n",
    "print(f\"   Checkpoint dir: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Import Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DEVICE, NUM_EPOCHS, BATCH_SIZE, LAMBDA_CONSISTENCY\n",
    "from model import create_encoder\n",
    "from data import create_data_loaders\n",
    "from train import Trainer\n",
    "from evaluate import Evaluator\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Device:              {DEVICE}\")\n",
    "print(f\"Default Epochs:      {NUM_EPOCHS}\")\n",
    "print(f\"Batch Size:          {BATCH_SIZE}\")\n",
    "print(f\"Lambda Consistency:  {LAMBDA_CONSISTENCY}\")\n",
    "print(\"\\n‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: QUICK TEST ‚Äî Synthetic Data (1 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"QUICK TEST ‚Äî Synthetic Data (2 epochs)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load synthetic data\n",
    "print(\"\\n[1/4] Loading synthetic SSVEP data...\")\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    dataset_name=\"synthetic\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Create model\n",
    "print(\"[2/4] Creating CNN encoder...\")\n",
    "model = create_encoder(encoder_type=\"cnn\")\n",
    "\n",
    "# Train (quick test: 2 epochs)\n",
    "print(\"[3/4] Training on GPU (2 epochs)...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    lambda_consistency=LAMBDA_CONSISTENCY,\n",
    "    num_epochs=2,  # Quick test\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    log_dir=os.path.join(CHECKPOINT_DIR, '..', 'logs'),\n",
    ")\n",
    "\n",
    "history_quick = trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n[4/4] Evaluating on test set...\")\n",
    "evaluator = Evaluator(model, test_loader, device=DEVICE)\n",
    "metrics_quick = evaluator.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUICK TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Test Accuracy:    {metrics_quick['accuracy']:.4f}\")\n",
    "print(f\"‚úÖ F1 Score:         {metrics_quick['f1_score']:.4f}\")\n",
    "print(f\"‚úÖ ITR:              {metrics_quick['itr']:.2f} bits/min\")\n",
    "print(\"\\n‚úÖ GPU training works! Ready for full training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: FULL TRAINING ‚Äî 500 Epochs (30-45 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FULL TRAINING ‚Äî Synthetic Data (500 epochs)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Reload data and create fresh model\n",
    "print(\"\\n[1/4] Loading synthetic SSVEP data...\")\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    dataset_name=\"synthetic\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "print(\"[2/4] Creating fresh CNN encoder...\")\n",
    "model = create_encoder(encoder_type=\"cnn\")\n",
    "\n",
    "# Train full (500 epochs with early stopping)\n",
    "print(\"[3/4] Training on GPU (up to 500 epochs with early stopping)...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    lambda_consistency=LAMBDA_CONSISTENCY,\n",
    "    num_epochs=500,  # Full training\n",
    "    device=DEVICE,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    log_dir=os.path.join(CHECKPOINT_DIR, '..', 'logs'),\n",
    ")\n",
    "\n",
    "history_full = trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n[4/4] Evaluating on test set...\")\n",
    "evaluator = Evaluator(model, test_loader, device=DEVICE)\n",
    "metrics_full = evaluator.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FULL TRAINING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Best Validation Accuracy:  {history_full['best_val_accuracy']:.4f}\")\n",
    "print(f\"Best Epoch:                {history_full['best_epoch']}\")\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  Accuracy:                {metrics_full['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score:                {metrics_full['f1_score']:.4f}\")\n",
    "print(f\"  ITR:                     {metrics_full['itr']:.2f} bits/min\")\n",
    "print(f\"  Within-class Distance:   {metrics_full['within_class_distance']:.6f}\")\n",
    "print(f\"  Between-class Distance:  {metrics_full['between_class_distance']:.6f}\")\n",
    "print(f\"  Consistency Ratio:       {metrics_full['consistency_ratio']:.4f}\")\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: ABLATION STUDIES ‚Äî Lambda Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ABLATION STUDIES ‚Äî Lambda Consistency\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lambda_values = [0.0, 0.01, 0.1, 1.0]\n",
    "ablation_results = {}\n",
    "\n",
    "for lambda_val in lambda_values:\n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"Lambda = {lambda_val:.2f}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    # Reload data\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        dataset_name=\"synthetic\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Create fresh model\n",
    "    model = create_encoder(encoder_type=\"cnn\")\n",
    "    \n",
    "    # Train with different lambda\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        lambda_consistency=lambda_val,\n",
    "        num_epochs=100,  # Shorter for ablation\n",
    "        device=DEVICE,\n",
    "        checkpoint_dir=CHECKPOINT_DIR,\n",
    "        log_dir=os.path.join(CHECKPOINT_DIR, '..', 'logs'),\n",
    "    )\n",
    "    \n",
    "    history = trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = Evaluator(model, test_loader, device=DEVICE)\n",
    "    metrics = evaluator.evaluate()\n",
    "    \n",
    "    ablation_results[lambda_val] = {\n",
    "        \"best_val_accuracy\": history[\"best_val_accuracy\"],\n",
    "        \"best_epoch\": history[\"best_epoch\"],\n",
    "        \"test_accuracy\": metrics[\"accuracy\"],\n",
    "        \"test_f1\": metrics[\"f1_score\"],\n",
    "        \"test_itr\": metrics[\"itr\"],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Œª={lambda_val:.2f} ‚Üí Val Acc: {history['best_val_accuracy']:.4f}, Test Acc: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "# Print ablation summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ABLATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nLambda  | Best Val Acc | Test Acc | Best Epoch\")\n",
    "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "for lam, res in sorted(ablation_results.items()):\n",
    "    print(f\"{lam:6.2f} | {res['best_val_accuracy']:12.4f} | {res['test_accuracy']:8.4f} | {res['best_epoch']:10d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING RESULTS TO GOOGLE DRIVE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save full training results\n",
    "full_results = {\n",
    "    \"experiment\": \"Neural Conditional Ensemble Averaging\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"device\": DEVICE,\n",
    "    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"none\",\n",
    "    \"training\": {\n",
    "        \"best_val_accuracy\": float(history_full[\"best_val_accuracy\"]),\n",
    "        \"best_epoch\": int(history_full[\"best_epoch\"]),\n",
    "        \"num_epochs_trained\": int(history_full.get(\"epoch\", 500)),\n",
    "        \"lambda_consistency\": float(LAMBDA_CONSISTENCY),\n",
    "    },\n",
    "    \"test_metrics\": {\n",
    "        \"accuracy\": float(metrics_full[\"accuracy\"]),\n",
    "        \"f1_score\": float(metrics_full[\"f1_score\"]),\n",
    "        \"itr\": float(metrics_full[\"itr\"]),\n",
    "        \"within_class_distance\": float(metrics_full.get(\"within_class_distance\", 0)),\n",
    "        \"between_class_distance\": float(metrics_full.get(\"between_class_distance\", 0)),\n",
    "        \"consistency_ratio\": float(metrics_full.get(\"consistency_ratio\", 0)),\n",
    "    },\n",
    "    \"ablation_studies\": {\n",
    "        str(lam): {k: float(v) if isinstance(v, (int, float)) else v for k, v in res.items()}\n",
    "        for lam, res in ablation_results.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to Drive\n",
    "results_file = os.path.join(RESULTS_DIR, 'colab_results.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(full_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved: {results_file}\")\n",
    "\n",
    "# Also save best checkpoint\n",
    "best_checkpoint = os.path.join(CHECKPOINT_DIR, 'best_model.pt')\n",
    "if os.path.exists(best_checkpoint):\n",
    "    shutil.copy2(best_checkpoint, os.path.join(RESULTS_DIR, 'best_model.pt'))\n",
    "    print(f\"‚úÖ Best checkpoint saved to Drive\")\n",
    "\n",
    "# Create a summary markdown file\n",
    "summary_md = f\"\"\"# üéâ Colab Training Complete\n",
    "\n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Device:** {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
    "\n",
    "## Full Training (500 epochs)\n",
    "- **Best Validation Accuracy:** {history_full['best_val_accuracy']:.4f}\n",
    "- **Best Epoch:** {history_full['best_epoch']}\n",
    "- **Test Accuracy:** {metrics_full['accuracy']:.4f}\n",
    "- **Test F1 Score:** {metrics_full['f1_score']:.4f}\n",
    "- **Test ITR:** {metrics_full['itr']:.2f} bits/min\n",
    "\n",
    "## Ablation Results\n",
    "\n",
    "| Lambda | Val Acc | Test Acc | Best Epoch |\n",
    "|--------|---------|----------|------------|\n",
    "\"\"\"\n",
    "\n",
    "for lam, res in sorted(ablation_results.items()):\n",
    "    summary_md += f\"| {lam:.2f} | {res['best_val_accuracy']:.4f} | {res['test_accuracy']:.4f} | {res['best_epoch']} |\\n\"\n",
    "\n",
    "summary_file = os.path.join(RESULTS_DIR, 'SUMMARY.md')\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "print(f\"‚úÖ Summary saved: {summary_file}\")\n",
    "print(f\"\\nüìÇ All results available at: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Display Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ COLAB TRAINING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ Pre-flight Validation:     PASSED (Guardian)\n",
    "‚úÖ Quick Test (2 epochs):     {metrics_quick['accuracy']:.4f} accuracy\n",
    "‚úÖ Full Training (500 epochs):{history_full['best_val_accuracy']:.4f} best val accuracy\n",
    "‚úÖ Ablation Studies:          4 lambda values tested\n",
    "‚úÖ Results Saved:             Google Drive/ncea_results/\n",
    "\n",
    "üìä FINAL TEST METRICS\n",
    "   Accuracy:     {metrics_full['accuracy']:.4f}\n",
    "   F1 Score:     {metrics_full['f1_score']:.4f}\n",
    "   ITR:          {metrics_full['itr']:.2f} bits/min\n",
    "\n",
    "üìÅ Drive Path: {RESULTS_DIR}\n",
    "üìù Summary:    {summary_file}\n",
    "üíæ Data:       {results_file}\n",
    "ü§ñ Model:      {RESULTS_DIR}/best_model.pt\n",
    "\n",
    "Next Steps:\n",
    "  1. Download results from Google Drive\n",
    "  2. Compare with baseline methods (TRCA, CNN, Li et al. 2024)\n",
    "  3. Generate paper figures and tables\n",
    "  4. Write Results section\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Load Real BETA Dataset\n",
    "\n",
    "If you have the BETA dataset, uncomment and run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download BETA dataset (optional)\n",
    "# # !wget https://github.com/gumpy-bci/data/raw/master/BETA/BETA.mat -O /content/ncea_denoise/workspace/data/BETA.mat\n",
    "# # print(\"‚úÖ BETA dataset downloaded\")\n",
    "# \n",
    "# # Then run training with BETA instead of synthetic:\n",
    "# # train_loader, val_loader, test_loader = create_data_loaders(\n",
    "# #     dataset_name=\"BETA\",\n",
    "# #     batch_size=BATCH_SIZE,\n",
    "# # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
