# ğŸš€ START HERE: Neural Conditional Ensemble Averaging

Welcome! This guide shows you exactly how to get your implementation running in **5 minutes**.

---

## What You Have

âœ… **Complete implementation** (3,000+ lines of production code)
âœ… **Full theory** (formalized with mathematical proofs)
âœ… **Comprehensive tests** (all modules verified)
âœ… **Guardian validation** (catches errors before expensive runs)
âœ… **Ready for GPU** (Colab setup included)

---

## Quick Start: 3 Steps

### Step 1: Validate (1 minute) ğŸ›¡ï¸

**Always run Guardian first!**

```bash
cd /Users/paul/prj/GenAI/vibe/paper_machine/auto-research-fleet
source venv/bin/activate
python3 workspace/src/guardian.py
```

You should see:
```
âœ“ ALL CHECKS PASSED!
Ready for real training runs.
```

### Step 2: Test Locally (2 minutes) ğŸ§ª

**Quick test on synthetic data:**

```bash
cd workspace/src
python3 main.py --mode train --dataset synthetic --epochs 2
```

Expected output:
```
Epoch 1/2: Loss 2.39 â†’ Val Accuracy 95-100%
Epoch 2/2: Loss 1.27 â†’ Val Accuracy 95-100%
```

### Step 3: Train on Real Data (Choose one) ğŸš€

#### Option A: **Colab GPU** (Recommended) âš¡
1. Open `COLAB_GPU_SETUP.md`
2. Copy the complete notebook
3. Go to https://colab.research.google.com/
4. Paste â†’ Run
5. Results auto-save to Google Drive

**Benefits**: Free GPU, 10-20x faster, no setup needed

#### Option B: **Local CPU**
```bash
cd workspace/src
python3 main.py --mode train --dataset BETA --epochs 50
# â±ï¸ Time: ~8-10 minutes on CPU
```

#### Option C: **Local GPU** (if you have one)
```bash
cd workspace/src
python3 main.py --mode train --dataset BETA --epochs 50 --device cuda
# â±ï¸ Time: ~2-3 minutes on GPU
```

---

## Understanding the Workflow

```
guardian.py (Validate)
    â†“
main.py --dataset synthetic (Test)
    â†“
main.py --dataset BETA (Real)
    â†“
Results!
```

**âš ï¸ DO NOT skip Guardian.** It saves hours of wasted computation.

---

## Comprehensive Integrity Verification (NEW!)

Use `verify_everything.py` to run **THREE LAYERS** of validation:

### Layer 1: Guardian (7 checks)
1. âœ“ All imports work
2. âœ“ Configuration is valid
3. âœ“ Model builds (546K params)
4. âœ“ Loss functions compute
5. âœ“ Data loads correctly
6. âœ“ Training step works
7. âœ“ CONFIG-SYNC is consistent

### Layer 2: Data Integrity (8 checks)
- âœ“ Data is real (not synthetic when claiming real)
- âœ“ No all-zeros trials
- âœ“ No constant values
- âœ“ Realistic signal statistics
- âœ“ Noise is present
- âœ“ Trials are different
- âœ“ Valid class distribution
- âœ“ No data leakage

### Layer 3: Forbidden Checks (Fraud Prevention)
- âœ“ NO synthetic data claimed as real
- âœ“ NO 100% accuracy on real data (HALLUCINATED)
- âœ“ NO data leakage between train/test
- âœ“ NO cherry-picked results
- âœ“ NO hand-coded metrics
- âœ“ Evidence saved (checkpoints, logs)

**Takes 3 minutes. Prevents hours of wasted GPU time and scientific fraud.**

---

## Key Commands Reference

| Command | Purpose | Time |
|---------|---------|------|
| `verify_everything.py --dataset synthetic` | Comprehensive validation (NEW!) | 3 min |
| `verify_everything.py --dataset BETA` | Validate with real data (NEW!) | 3 min |
| `guardian.py` | Guardian checks only | 1 min |
| `main.py --mode train --dataset synthetic` | Test on fake data | 2 min |
| `main.py --mode train --dataset BETA` | Train on real SSVEP | 10 min (GPU) / 1 hour (CPU) |
| `main.py --mode ablation` | Test different Î» values | 30 min (GPU) |
| `main.py --mode compare` | Compare TRCA vs CNN vs Proposed | 30 min (GPU) |

---

## File Structure

```
auto-research-fleet/
â”œâ”€â”€ START_HERE.md                    â† You are here!
â”œâ”€â”€ QUICK_START.md                   â† Quick reference
â”œâ”€â”€ GUARDIAN_GUIDE.md                â† Guardian documentation
â”œâ”€â”€ DATA_INTEGRITY.md                â† Fraud prevention guide
â”œâ”€â”€ VERIFY_WORKFLOW.md               â† Verification system guide (NEW!)
â”œâ”€â”€ INTEGRITY_SYSTEM.md              â† Architecture overview (NEW!)
â”œâ”€â”€ COLAB_GPU_SETUP.md               â† Colab notebook (copy-paste)
â”‚
â”œâ”€â”€ workspace/src/                   â† ALL CODE HERE
â”‚   â”œâ”€â”€ main.py                      â† Entry point
â”‚   â”œâ”€â”€ guardian.py                  â† Pre-flight validation
â”‚   â”œâ”€â”€ data_integrity.py            â† Real vs. fake data detection
â”‚   â”œâ”€â”€ forbidden_checks.py          â† Fraud prevention checks
â”‚   â”œâ”€â”€ verify_everything.py         â† Unified orchestrator (NEW!)
â”‚   â”œâ”€â”€ config.py                    â† All hyperparameters
â”‚   â”œâ”€â”€ model.py                     â† Neural network (546K params)
â”‚   â”œâ”€â”€ train.py                     â† Training loop
â”‚   â”œâ”€â”€ evaluate.py                  â† Evaluation
â”‚   â”œâ”€â”€ losses.py                    â† Consistency loss
â”‚   â”œâ”€â”€ data.py                      â† Data loading
â”‚   â”œâ”€â”€ metrics.py                   â† 8 evaluation metrics
â”‚   â””â”€â”€ baselines.py                 â† TRCA, CNN baselines
â”‚
â”œâ”€â”€ workspace/paper/                 â† LaTeX paper
â”‚   â”œâ”€â”€ main.tex                     â† (Sections I-III done)
â”‚   â”œâ”€â”€ related_work.bib             â† 48 papers
â”‚   â””â”€â”€ theory_formalization.md      â† Math framework
â”‚
â””â”€â”€ venv/                            â† Virtual environment (ready!)
```

---

## Expected Accuracies

| Method | Dataset | Accuracy |
|--------|---------|----------|
| TRCA (baseline) | Synthetic | 85-90% |
| CNN (baseline) | Synthetic | 88-95% |
| **Your Method** | Synthetic | 95-100% âœ“ |
| TRCA | BETA | 88-92% |
| CNN | BETA | 90-94% |
| **Your Method** | BETA | 92-96% (target) |

---

## Troubleshooting

### "Guardian failed"
â†’ Read the error message, fix it, run Guardian again

### "Training is slow"
â†’ Use Colab GPU (10-20x faster than CPU)

### "Out of memory"
â†’ Reduce `BATCH_SIZE` in config.py (32 â†’ 16 â†’ 8)

### "Data not found"
â†’ Use synthetic data for testing (`--dataset synthetic`)

### "Import error"
â†’ Check venv is activated: `source venv/bin/activate`

---

## Next Steps

### For Quick Testing (5 min)
```bash
source venv/bin/activate
python3 workspace/src/guardian.py  # Validate
cd workspace/src
python3 main.py --mode train --dataset synthetic --epochs 2  # Test
```

### For Real Experiments (30 min setup, 2 hours training)
1. Read `COLAB_GPU_SETUP.md`
2. Copy notebook code
3. Go to Colab
4. Paste & Run
5. Results auto-save to Google Drive

### For Paper Results (3-4 hours)
```bash
python3 main.py --mode compare    # Compare methods
python3 main.py --mode ablation   # Ablation studies
# Save results â†’ Generate figures â†’ Write paper
```

---

## Important: Always Run Guardian!

**Before any real training:**
```bash
python3 guardian.py
# Wait for: "âœ“ ALL CHECKS PASSED!"
```

Guardian catches:
- âœ“ Import errors
- âœ“ Configuration mistakes
- âœ“ Architecture bugs
- âœ“ Loss computation issues
- âœ“ Data loading problems
- âœ“ Training failures

**Cost**: 1 minute
**Benefit**: Save 2+ hours of wasted GPU time

---

## Colab vs Local

| Feature | Colab | Local |
|---------|-------|-------|
| **GPU** | Free T4/V100 | Need to set up |
| **Speed** | 10-20x faster | Slower |
| **Setup** | 2 minutes | Already done |
| **Storage** | Google Drive | Your disk |
| **Time limit** | 12 hours | Unlimited |
| **Best for** | Training | Development |

**Recommendation**: Use Colab for real training, local for testing.

---

## The Validation Workflow

```
1. guardian.py
   â†“
   [âœ“ All checks pass?]

   â”œâ”€ NO  â†’ Fix issues â†’ Run guardian.py again
   â””â”€ YES â†’ Proceed

2. main.py --dataset synthetic (2 epochs)
   â†“
   [âœ“ Training works?]

   â”œâ”€ NO  â†’ Debug â†’ Run guardian.py again
   â””â”€ YES â†’ Proceed

3. main.py --dataset BETA (50 epochs)
   â†“
   [âœ“ Results look good?]

   â”œâ”€ NO  â†’ Adjust hyperparameters â†’ Run guardian.py again
   â””â”€ YES â†’ Done! Write paper.
```

---

## One-Minute Checklist Before Training

- [ ] Run Guardian: `python3 guardian.py` â† **ALWAYS DO THIS FIRST**
- [ ] Verify all checks pass (green âœ“)
- [ ] Confirm dataset valid (BETA, OpenBMI, synthetic)
- [ ] Check batch size reasonable (8-32)
- [ ] Verify GPU available if using Colab

---

## Files You Should Know

| File | Purpose | Edit? |
|------|---------|-------|
| `guardian.py` | Validation | No |
| `config.py` | Hyperparameters | **YES** (if changing settings) |
| `main.py` | Training entry | No |
| `COLAB_GPU_SETUP.md` | Colab notebook | No (copy-paste) |

---

## Support

- **Quick reference**: `QUICK_START.md`
- **Guardian help**: `GUARDIAN_GUIDE.md`
- **Data integrity**: `DATA_INTEGRITY.md`
- **Verification system**: `VERIFY_WORKFLOW.md` (NEW!)
- **System architecture**: `INTEGRITY_SYSTEM.md` (NEW!)
- **Colab setup**: `COLAB_GPU_SETUP.md`
- **Theory**: `workspace/paper/theory_formalization.md`
- **Full docs**: `IMPLEMENTATION_COMPLETE.md`

---

## TL;DR

```bash
# 1. Validate
python3 guardian.py

# 2. Test
python3 main.py --mode train --dataset synthetic --epochs 2

# 3. Train (pick one)
# Option A: Colab (recommended)
# â†’ See COLAB_GPU_SETUP.md

# Option B: Local
python3 main.py --mode train --dataset BETA --epochs 50
```

**That's it! Your implementation is ready.** ğŸš€

---

## Ready?

Pick your next action:

1. **Validate**: `python3 guardian.py`
2. **Test locally**: `python3 main.py --mode train --dataset synthetic`
3. **Train on Colab**: Copy notebook from `COLAB_GPU_SETUP.md`
4. **Learn more**: Read `GUARDIAN_GUIDE.md` or `QUICK_START.md`

**Go!** ğŸ’ª

